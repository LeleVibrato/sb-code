{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import spacy\n",
    "import random\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# set the random seed for reproducibility\n",
    "random.seed(123)\n",
    "# turn off depreciation warnings and future warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# load spacy model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# function to load data\n",
    "def load_data(base_dir):\n",
    "    data = []\n",
    "    labels = []\n",
    "    files = []\n",
    "    for label in ['positive', 'negative']:\n",
    "        for filepath in glob.glob(os.path.join(base_dir, label, '*.txt')):\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                data.append(file.read())\n",
    "                labels.append(1 if label == 'positive' else 0)\n",
    "                files.append(filepath)\n",
    "                \n",
    "    return data, labels, files\n",
    "\n",
    "# delete the contents after \"What I've decided and why\"\n",
    "def clean_data(data):\n",
    "    cleaned_data = []\n",
    "    for text in data:\n",
    "        cleaned_data.append(text.split(\"What I've decided and why\")[0])\n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vfidf vectorizer and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the texts\n",
    "def preprocess_texts(texts):\n",
    "    docs = [nlp(text) for text in texts]\n",
    "    return docs\n",
    "\n",
    "# function to remove stopwords and punctuation\n",
    "def remove_stopwords_punctuation(docs):\n",
    "    cleaned_docs = []\n",
    "    for doc in docs:\n",
    "        doc = [token for token in doc if not token.is_stop and not token.is_punct]\n",
    "        doc = [token for token in doc if token.text not in ['\\n', 'Mr', 'Mrs', 'Miss', 'Ms']]\n",
    "        doc = [token for token in doc if len(token.text) > 1]\n",
    "        cleaned_docs.append(doc)\n",
    "    return cleaned_docs\n",
    "\n",
    "#  lowercase and lemmatise the tokens\n",
    "def lowercase_and_lemmatise(docs):\n",
    "    lemmatised_docs = []\n",
    "    for doc in docs:\n",
    "        lemmatised_tokens = [token.lemma_.lower() for token in doc]\n",
    "        lemmatised_docs.append(lemmatised_tokens)\n",
    "    return lemmatised_docs\n",
    "\n",
    "# join the tokens back together\n",
    "def join_tokens(docs):\n",
    "    return [' '.join(doc) for doc in docs]\n",
    "\n",
    "# load training data\n",
    "train_data, train_labels, train_files = load_data('data/train')\n",
    "# print the number of training samples\n",
    "print(f'Number of training samples: {len(train_data)}')\n",
    "# load test data\n",
    "test_data, test_labels, test_files = load_data('data/test')\n",
    "# print the number of test samples\n",
    "print(f'Number of test samples: {len(test_data)}')\n",
    "\n",
    "train_data = clean_data(train_data)\n",
    "test_data = clean_data(test_data)\n",
    "\n",
    "# preprocess the training data\n",
    "train_data = preprocess_texts(train_data)\n",
    "# preprocess the test data\n",
    "test_data = preprocess_texts(test_data)\n",
    "\n",
    "# remove stopwords and punctuation from the training data\n",
    "train_data = remove_stopwords_punctuation(train_data)\n",
    "# remove stopwords and punctuation from the test data\n",
    "test_data = remove_stopwords_punctuation(test_data)\n",
    "\n",
    "# lowercase and lemmatise the training data\n",
    "train_data = lowercase_and_lemmatise(train_data)\n",
    "# lowercase and lemmatise the test data\n",
    "test_data = lowercase_and_lemmatise(test_data)\n",
    "\n",
    "# join the tokens back together for the training data\n",
    "train_data = join_tokens(train_data)\n",
    "# join the tokens back together for the test data\n",
    "test_data = join_tokens(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Evaluate model and return metrics\n",
    "def evaluate_model(model, test_data, test_labels, data_type='test'):\n",
    "    predictions = model.predict(test_data)\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    precision = precision_score(test_labels, predictions)\n",
    "    recall = recall_score(test_labels, predictions)\n",
    "    f1 = f1_score(test_labels, predictions)\n",
    "    conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "    print(f'{data_type} data metrics:')\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(f'Confusion Matrix: \\n{conf_matrix}')\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# create a pipeline with TfidfVectorizer and LogisticRegression\n",
    "logreg_text_clf_pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    LogisticRegression(max_iter=1000, random_state=123)\n",
    ")\n",
    "\n",
    "# define parameter grid for GridSearchCV\n",
    "param_grid_logreg = {\n",
    "    'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'tfidfvectorizer__max_df': [0.9, 0.95],\n",
    "    'tfidfvectorizer__min_df': [2, 5],\n",
    "    'logisticregression__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# perform GridSearchCV\n",
    "grid_search_logreg = GridSearchCV(logreg_text_clf_pipeline, param_grid_logreg, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_logreg.fit(train_data, train_labels)\n",
    "\n",
    "# best model\n",
    "best_logreg_model = grid_search_logreg.best_estimator_\n",
    "\n",
    "# print the best parameters\n",
    "print(grid_search_logreg.best_params_)\n",
    "\n",
    "# evaluate the best model on the training data\n",
    "train_metrics_logreg = evaluate_model(best_logreg_model, train_data, train_labels, data_type='train')\n",
    "\n",
    "# evaluate the best model on the test data\n",
    "test_metrics_logreg = evaluate_model(best_logreg_model, test_data, test_labels, data_type='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vfidf vectorizer and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a pipeline with TfidfVectorizer and SVC\n",
    "svm_text_clf_pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SVC(probability=True, random_state=123)\n",
    ")\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid_svc = {\n",
    "    'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'tfidfvectorizer__max_df': [0.9, 0.95],\n",
    "    'tfidfvectorizer__min_df': [2, 5],\n",
    "    'svc__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search_svc = GridSearchCV(svm_text_clf_pipeline, param_grid_svc, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_svc.fit(train_data, train_labels)\n",
    "\n",
    "# Best model\n",
    "best_svc_model = grid_search_svc.best_estimator_\n",
    "\n",
    "# Print the best parameters\n",
    "print(grid_search_svc.best_params_)\n",
    "\n",
    "# Get model metrics\n",
    "train_metrics_svc = evaluate_model(best_svc_model, train_data, train_labels, data_type='Train')\n",
    "print('-'*50)\n",
    "test_metrics_svc = evaluate_model(best_svc_model, test_data, test_labels, data_type='Test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vfidf vectorizer and RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a pipeline with TfidfVectorizer and RandomForestClassifier\n",
    "rf_text_clf_pipeline = make_pipeline(\n",
    "    TfidfVectorizer(decode_error='ignore'),\n",
    "    RandomForestClassifier(n_jobs=-1, random_state=123)\n",
    ")\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid_rf = {\n",
    "    'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'tfidfvectorizer__max_df': [0.9, 0.95],\n",
    "    'tfidfvectorizer__min_df': [2, 5],\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search_rf = GridSearchCV(rf_text_clf_pipeline, param_grid_rf, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_rf.fit(train_data, train_labels)\n",
    "\n",
    "# Best model\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "\n",
    "# Print the best parameters\n",
    "print(grid_search_rf.best_params_)\n",
    "# Get model metrics\n",
    "train_metrics_rf = evaluate_model(best_rf_model, train_data, train_labels, data_type='Train')\n",
    "print('-'*50)\n",
    "test_metrics_rf = evaluate_model(best_rf_model, test_data, test_labels, data_type='Test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to plot the roc curve\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# function returns fpr, tpr and auc\n",
    "def get_roc_curve(model, test_data, test_labels):\n",
    "    probabilities = model.predict_proba(test_data)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(test_labels, probabilities)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    return fpr, tpr, auc_score\n",
    "\n",
    "# plot the roc curve\n",
    "def plot_roc_curve(models, model_names, test_data, test_labels, data_type='test'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for model, model_name in zip(models, model_names):\n",
    "        fpr, tpr, auc_score = get_roc_curve(model, test_data, test_labels)\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    # ratio set to 'equal' to ensure the aspect ratio is equal\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve ({data_type} data)')\n",
    "    plt.legend()\n",
    "    # save the plot\n",
    "    if not os.path.exists('plots'):\n",
    "        os.makedirs('plots')\n",
    "    plt.savefig(f'plots/roc_curve_{data_type}.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# plot the roc curve\n",
    "models = [best_logreg_model, best_svc_model, best_rf_model]\n",
    "model_names = ['Logistic Regression', 'SVC', 'Random Forest']\n",
    "plot_roc_curve(models, model_names, train_data, train_labels, data_type='train')\n",
    "plot_roc_curve(models, model_names, test_data, test_labels, data_type='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Importance\n",
    "# Get the feature names from the TfidfVectorizer\n",
    "feature_names = best_rf_model.named_steps['tfidfvectorizer'].get_feature_names_out()\n",
    "\n",
    "# Get the feature importances from the RandomForestClassifier\n",
    "feature_importances = best_rf_model.named_steps['randomforestclassifier'].feature_importances_\n",
    "\n",
    "# Create a DataFrame with the feature names and feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importances\n",
    "})\n",
    "\n",
    "# Get the top 20 features\n",
    "top_20_features = feature_importance_df.sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "# Plot the top 20 features\n",
    "def plot_feature_importance(feature_df, title):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_df, palette='viridis')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    # save the plot\n",
    "    if not os.path.exists('plots'):\n",
    "        os.makedirs('plots')\n",
    "    plt.savefig(f'plots/{title}.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# plot the top 20 features\n",
    "plot_feature_importance(top_20_features, 'Top 20 Features - Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
