{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import spacy\n",
    "import random\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# set the random seed for reproducibility\n",
    "random.seed(123)\n",
    "# turn off depreciation warnings and future warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# load spacy model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# function to load data\n",
    "def load_data(base_dir):\n",
    "    data = []\n",
    "    labels = []\n",
    "    files = []\n",
    "    for label in ['positive', 'negative']:\n",
    "        for filepath in glob.glob(os.path.join(base_dir, label, '*.txt')):\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                data.append(file.read())\n",
    "                labels.append(1 if label == 'positive' else 0)\n",
    "                files.append(filepath)\n",
    "                \n",
    "    return data, labels, files\n",
    "\n",
    "# delete the contents after \"What I've decided and why\"\n",
    "def clean_data(data):\n",
    "    cleaned_data = []\n",
    "    for text in data:\n",
    "        cleaned_data.append(text.split(\"What I've decided and why\")[0])\n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vfidf vectorizer and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the texts\n",
    "def preprocess_texts(texts):\n",
    "    docs = [nlp(text) for text in texts]\n",
    "    return docs\n",
    "\n",
    "# function to remove stopwords and punctuation\n",
    "def remove_stopwords_punctuation(docs):\n",
    "    cleaned_docs = []\n",
    "    for doc in docs:\n",
    "        doc = [token for token in doc if not token.is_stop and not token.is_punct]\n",
    "        doc = [token for token in doc if token.text not in ['\\n', 'Mr', 'Mrs', 'Miss', 'Ms']]\n",
    "        doc = [token for token in doc if len(token.text) > 1]\n",
    "        cleaned_docs.append(doc)\n",
    "    return cleaned_docs\n",
    "\n",
    "#  lowercase and lemmatise the tokens\n",
    "def lowercase_and_lemmatise(docs):\n",
    "    lemmatised_docs = []\n",
    "    for doc in docs:\n",
    "        lemmatised_tokens = [token.lemma_.lower() for token in doc]\n",
    "        lemmatised_docs.append(lemmatised_tokens)\n",
    "    return lemmatised_docs\n",
    "\n",
    "# join the tokens back together\n",
    "def join_tokens(docs):\n",
    "    return [' '.join(doc) for doc in docs]\n",
    "\n",
    "# load training data\n",
    "train_data, train_labels, train_files = load_data('data/train')\n",
    "# load test data\n",
    "test_data, test_labels, test_files = load_data('data/test')\n",
    "\n",
    "train_data = clean_data(train_data)\n",
    "test_data = clean_data(test_data)\n",
    "\n",
    "# preprocess the training data\n",
    "train_data = preprocess_texts(train_data)\n",
    "# preprocess the test data\n",
    "test_data = preprocess_texts(test_data)\n",
    "\n",
    "# remove stopwords and punctuation from the training data\n",
    "train_data = remove_stopwords_punctuation(train_data)\n",
    "# remove stopwords and punctuation from the test data\n",
    "test_data = remove_stopwords_punctuation(test_data)\n",
    "\n",
    "# lowercase and lemmatise the training data\n",
    "train_data = lowercase_and_lemmatise(train_data)\n",
    "# lowercase and lemmatise the test data\n",
    "test_data = lowercase_and_lemmatise(test_data)\n",
    "\n",
    "# join the tokens back together for the training data\n",
    "train_data = join_tokens(train_data)\n",
    "# join the tokens back together for the test data\n",
    "test_data = join_tokens(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Evaluate model and return metrics\n",
    "def evaluate_model(model, test_data, test_labels, data_type='test'):\n",
    "    predictions = model.predict(test_data)\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    precision = precision_score(test_labels, predictions)\n",
    "    recall = recall_score(test_labels, predictions)\n",
    "    f1 = f1_score(test_labels, predictions)\n",
    "    conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "    print(f'{data_type} data metrics:')\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(f'Confusion Matrix: \\n{conf_matrix}')\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "{'logisticregression__C': 10, 'tfidfvectorizer__max_df': 0.9, 'tfidfvectorizer__min_df': 5, 'tfidfvectorizer__ngram_range': (1, 3)}\n",
      "train data metrics:\n",
      "Accuracy: 0.9951\n",
      "Precision: 0.9927\n",
      "Recall: 0.9975\n",
      "F1 Score: 0.9951\n",
      "Confusion Matrix: \n",
      "[[409   3]\n",
      " [  1 406]]\n",
      "test data metrics:\n",
      "Accuracy: 0.7415\n",
      "Precision: 0.7526\n",
      "Recall: 0.7157\n",
      "F1 Score: 0.7337\n",
      "Confusion Matrix: \n",
      "[[79 24]\n",
      " [29 73]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# create a pipeline with TfidfVectorizer and LogisticRegression\n",
    "logreg_text_clf_pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    LogisticRegression(max_iter=1000)\n",
    ")\n",
    "\n",
    "# define parameter grid for GridSearchCV\n",
    "param_grid_logreg = {\n",
    "    'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'tfidfvectorizer__max_df': [0.9, 0.95],\n",
    "    'tfidfvectorizer__min_df': [2, 5],\n",
    "    'logisticregression__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# perform GridSearchCV\n",
    "grid_search_logreg = GridSearchCV(logreg_text_clf_pipeline, param_grid_logreg, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_logreg.fit(train_data, train_labels)\n",
    "\n",
    "# best model\n",
    "best_logreg_model = grid_search_logreg.best_estimator_\n",
    "\n",
    "# print the best parameters\n",
    "print(grid_search_logreg.best_params_)\n",
    "\n",
    "# evaluate the best model on the training data\n",
    "train_metrics_logreg = evaluate_model(best_logreg_model, train_data, train_labels, data_type='train')\n",
    "\n",
    "# evaluate the best model on the test data\n",
    "test_metrics_logreg = evaluate_model(best_logreg_model, test_data, test_labels, data_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # function to plot the most important unigrams and bigrams\n",
    "# def plot_top_coefficients(model, train_data, top_n=20):\n",
    "#     # fit the vectorizer to the training data to get feature names\n",
    "#     vectorizer = model.named_steps['tfidfvectorizer']\n",
    "#     X_train_transformed = vectorizer.fit_transform(train_data)\n",
    "\n",
    "#     # get the logistic regression model coefficients\n",
    "#     log_reg = model.named_steps['logisticregression']\n",
    "#     coefficients = log_reg.coef_.flatten()\n",
    "\n",
    "#     # get feature names (unigrams and bigrams)\n",
    "#     feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "#     # get the top positive and negative features (highest and lowest coefficients)\n",
    "#     top_positive_coefficients = np.argsort(coefficients)[-top_n:]\n",
    "#     top_negative_coefficients = np.argsort(coefficients)[:top_n]\n",
    "\n",
    "#     # plot the most important unigrams and bigrams\n",
    "#     top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
    "#     plt.figure(figsize=(15, 5))\n",
    "#     colors = ['lightcoral' if c < 0 else 'lightblue' for c in coefficients[top_coefficients]]\n",
    "#     plt.bar(np.arange(2 * top_n), coefficients[top_coefficients], color=colors)\n",
    "#     feature_names = np.array(feature_names)\n",
    "#     plt.xticks(np.arange(2 * top_n), feature_names[top_coefficients], rotation=60, ha='right')\n",
    "#     plt.title(f'top {top_n//2} positive and negative unigrams and bigrams')\n",
    "#     plt.show()\n",
    "\n",
    "# # plot the top coefficients using the best model and training data\n",
    "# plot_top_coefficients(best_logreg_model, train_data, top_n=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vfidf vectorizer and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "{'svc__C': 10, 'tfidfvectorizer__max_df': 0.9, 'tfidfvectorizer__min_df': 5, 'tfidfvectorizer__ngram_range': (1, 3)}\n",
      "Train data metrics:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix: \n",
      "[[412   0]\n",
      " [  0 407]]\n",
      "--------------------------------------------------\n",
      "Test data metrics:\n",
      "Accuracy: 0.7415\n",
      "Precision: 0.7634\n",
      "Recall: 0.6961\n",
      "F1 Score: 0.7282\n",
      "Confusion Matrix: \n",
      "[[81 22]\n",
      " [31 71]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a pipeline with TfidfVectorizer and SVC\n",
    "svm_text_clf_pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SVC()\n",
    ")\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid_svc = {\n",
    "    'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'tfidfvectorizer__max_df': [0.9, 0.95],\n",
    "    'tfidfvectorizer__min_df': [2, 5],\n",
    "    'svc__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search_svc = GridSearchCV(svm_text_clf_pipeline, param_grid_svc, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_svc.fit(train_data, train_labels)\n",
    "\n",
    "# Best model\n",
    "best_svc_model = grid_search_svc.best_estimator_\n",
    "\n",
    "# Print the best parameters\n",
    "print(grid_search_svc.best_params_)\n",
    "\n",
    "# Get model metrics\n",
    "train_metrics_svc = evaluate_model(best_svc_model, train_data, train_labels, data_type='Train')\n",
    "print('-'*50)\n",
    "test_metrics_svc = evaluate_model(best_svc_model, test_data, test_labels, data_type='Test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "{'randomforestclassifier__n_estimators': 100, 'tfidfvectorizer__max_df': 0.95, 'tfidfvectorizer__min_df': 5, 'tfidfvectorizer__ngram_range': (1, 3)}\n",
      "Train data metrics:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix: \n",
      "[[412   0]\n",
      " [  0 407]]\n",
      "--------------------------------------------------\n",
      "Test data metrics:\n",
      "Accuracy: 0.7171\n",
      "Precision: 0.7157\n",
      "Recall: 0.7157\n",
      "F1 Score: 0.7157\n",
      "Confusion Matrix: \n",
      "[[74 29]\n",
      " [29 73]]\n"
     ]
    }
   ],
   "source": [
    "## Vfidf vectorizer and RandomForest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a pipeline with TfidfVectorizer and RandomForestClassifier\n",
    "rf_text_clf_pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid_rf = {\n",
    "    'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'tfidfvectorizer__max_df': [0.9, 0.95],\n",
    "    'tfidfvectorizer__min_df': [2, 5],\n",
    "    'randomforestclassifier__n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search_rf = GridSearchCV(rf_text_clf_pipeline, param_grid_rf, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_rf.fit(train_data, train_labels)\n",
    "\n",
    "# Best model\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "\n",
    "# Print the best parameters\n",
    "print(grid_search_rf.best_params_)\n",
    "# Get model metrics\n",
    "train_metrics_rf = evaluate_model(best_rf_model, train_data, train_labels, data_type='Train')\n",
    "print('-'*50)\n",
    "test_metrics_rf = evaluate_model(best_rf_model, test_data, test_labels, data_type='Test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
