{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pypdf import PdfReader\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 10 entries from page 0\n",
      "Scraping 10 entries from page 10\n",
      "Scraping 10 entries from page 20\n",
      "Scraping 10 entries from page 30\n",
      "Scraping 10 entries from page 40\n",
      "Scraping 10 entries from page 50\n",
      "Scraping 10 entries from page 60\n",
      "Scraping 10 entries from page 70\n",
      "Scraping 10 entries from page 80\n",
      "Scraping 10 entries from page 90\n",
      "Scraping 10 entries from page 100\n",
      "Scraping 10 entries from page 110\n",
      "Scraping 10 entries from page 120\n",
      "Scraping 10 entries from page 130\n",
      "Scraping 10 entries from page 140\n",
      "Scraping 10 entries from page 150\n",
      "Scraping 10 entries from page 160\n",
      "Scraping 10 entries from page 170\n",
      "Scraping 10 entries from page 180\n",
      "Scraping 10 entries from page 190\n",
      "Scraping 10 entries from page 200\n",
      "Scraping 10 entries from page 210\n",
      "Scraping 10 entries from page 220\n",
      "Scraping 10 entries from page 230\n",
      "Scraping 10 entries from page 240\n",
      "Scraping 10 entries from page 250\n",
      "Scraping 10 entries from page 260\n",
      "Scraping 10 entries from page 270\n",
      "Scraping 10 entries from page 280\n",
      "Scraping 10 entries from page 290\n",
      "Scraping 10 entries from page 300\n",
      "Scraping 10 entries from page 310\n",
      "Scraping 10 entries from page 320\n",
      "Scraping 10 entries from page 330\n",
      "Scraping 10 entries from page 340\n",
      "Scraping 10 entries from page 350\n",
      "Scraping 10 entries from page 360\n",
      "Scraping 10 entries from page 370\n",
      "Scraping 10 entries from page 380\n",
      "Scraping 10 entries from page 390\n",
      "Scraping 10 entries from page 400\n",
      "Scraping 10 entries from page 410\n",
      "Scraping 10 entries from page 420\n",
      "Scraping 10 entries from page 430\n",
      "Scraping 10 entries from page 440\n",
      "Scraping 10 entries from page 450\n",
      "Scraping 10 entries from page 460\n",
      "Scraping 10 entries from page 470\n",
      "Scraping 10 entries from page 480\n",
      "Scraping 10 entries from page 490\n",
      "Scraping 10 entries from page 500\n",
      "Scraping 10 entries from page 510\n",
      "Scraping 10 entries from page 520\n",
      "Scraping 10 entries from page 530\n",
      "Scraping 10 entries from page 540\n",
      "Scraping 10 entries from page 550\n",
      "Scraping 10 entries from page 560\n",
      "Scraping 10 entries from page 570\n",
      "Scraping 10 entries from page 580\n",
      "Scraping 10 entries from page 590\n",
      "Scraping 10 entries from page 600\n",
      "Scraping 10 entries from page 610\n",
      "Scraping 10 entries from page 620\n",
      "Scraping 10 entries from page 630\n",
      "Scraping 10 entries from page 640\n",
      "Scraping 10 entries from page 650\n",
      "Scraping 10 entries from page 660\n",
      "Scraping 10 entries from page 670\n",
      "Scraping 10 entries from page 680\n",
      "Scraping 10 entries from page 690\n",
      "Scraping 10 entries from page 700\n",
      "Scraping 10 entries from page 710\n",
      "Scraping 10 entries from page 720\n",
      "Scraping 10 entries from page 730\n",
      "Scraping 10 entries from page 740\n",
      "Scraping 10 entries from page 750\n",
      "Scraping 10 entries from page 760\n",
      "Scraping 10 entries from page 770\n",
      "Scraping 10 entries from page 780\n",
      "Scraping 10 entries from page 790\n",
      "Scraping 10 entries from page 800\n",
      "Scraping 10 entries from page 810\n",
      "Scraping 10 entries from page 820\n",
      "Scraping 10 entries from page 830\n",
      "Scraping 10 entries from page 840\n",
      "Scraping 10 entries from page 850\n",
      "Scraping 10 entries from page 860\n",
      "Scraping 10 entries from page 870\n",
      "Scraping 10 entries from page 880\n",
      "Scraping 10 entries from page 890\n",
      "Scraping 10 entries from page 900\n",
      "Scraping 10 entries from page 910\n",
      "Scraping 10 entries from page 920\n",
      "Scraping 10 entries from page 930\n",
      "Scraping 10 entries from page 940\n",
      "Scraping 10 entries from page 950\n",
      "Scraping 10 entries from page 960\n",
      "Scraping 10 entries from page 970\n",
      "Scraping 10 entries from page 980\n",
      "Scraping 10 entries from page 990\n",
      "Scraping 10 entries from page 1000\n",
      "Scraping 10 entries from page 1010\n",
      "Scraping 10 entries from page 1020\n",
      "Scraping 10 entries from page 1030\n",
      "Scraping 10 entries from page 1040\n",
      "Scraping 10 entries from page 1050\n",
      "Scraping 10 entries from page 1060\n",
      "Scraping 10 entries from page 1070\n",
      "Scraping 10 entries from page 1080\n",
      "Scraping 10 entries from page 1090\n",
      "Scraping 10 entries from page 1100\n",
      "Scraping 10 entries from page 1110\n",
      "Scraping 10 entries from page 1120\n",
      "Scraping 10 entries from page 1130\n",
      "Scraping 10 entries from page 1140\n",
      "Scraping 7 entries from page 1150\n",
      "Finished scraping at 1160\n",
      "Writing 1157 entries to metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# download the data\n",
    "%run scrape.py get-metadata --from 2024-04-01 --to 2024-06-30 --industry-sector insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run scrape.py download-decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and read the CSV file to get the location of the pdf files\n",
    "csv_path = 'metadata.csv'\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory to save TXT files\n",
    "output_dir = './plaintext'\n",
    "# if already exists, delete it\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "# create the directory\n",
    "os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract text from PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # delete the PDF identification\n",
    "    text = re.sub(r'^DRN-\\d+\\n', '', text)\n",
    "    # change \"’\" to \"'\"\n",
    "    text = re.sub(r'’', \"'\", text)\n",
    "    # delete the non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    # add a space after each period\n",
    "    text = re.sub(r'\\.([^\\s])', r'. \\1', text)\n",
    "    # delete the extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # add new lines before specific keywords\n",
    "    text = text.replace(\"The complaint \", 'The complaint\\n')\n",
    "    text = text.replace(\"What happened \", '\\nWhat happened\\n')\n",
    "    text = text.replace(\"What I've decided and why \", \"\\nWhat I've decided and why\\n\")\n",
    "    # only add a new line before \"My final decision\" if it is the first occurrence\n",
    "    text = re.sub(r'My final decision ', r'\\nMy final decision\\n', text, count=1)\n",
    "    # delete the last two lines of the text\n",
    "    text = re.sub(r'\\.\\s[^\\.\\n]*$', '.', text)\n",
    " \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each row to get the PDF file path from 'location' column\n",
    "for index, row in df.iterrows():\n",
    "    pdf_path = row['location']\n",
    "\n",
    "    # extract text from the PDF file\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        \n",
    "        # clean the text\n",
    "        text = clean_text(text)\n",
    "\n",
    "        # create a plaintext file with the same name as the PDF file\n",
    "        txt_filename = os.path.basename(pdf_path).replace('.pdf', '.txt')\n",
    "        txt_path = os.path.join(output_dir, txt_filename)\n",
    "        \n",
    "        # write the text to the file\n",
    "        with open(txt_path, 'w', encoding='utf-8') as txt_file:\n",
    "            txt_file.write(text)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crete a new column in the dataframe to store the path of the plaintext files\n",
    "fn = lambda x: os.path.join(output_dir[2:], os.path.basename(x).replace('.pdf', '.txt'))\n",
    "df['plaintext_path'] = df['location'].apply(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.2, stratify=df['decision'], random_state=42)\n",
    "# add a new column 'set' to indicate whether the sample is in the training or testing set\n",
    "train_data['set'] = 'train'\n",
    "test_data['set'] = 'test'\n",
    "# combine the training and testing data back into one dataframe\n",
    "df = pd.concat([train_data, test_data])\n",
    "# save the updated dataframe to a CSV file\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the plaintext files to the training and testing directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files have been organized successfully.\n"
     ]
    }
   ],
   "source": [
    "# read the metadata file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# define base directory\n",
    "base_dir = './data'\n",
    "\n",
    "# if the base directory already exists, delete it\n",
    "if os.path.exists(base_dir):\n",
    "    shutil.rmtree(base_dir)\n",
    "\n",
    "# define decision mapping\n",
    "decision_mapping = {\n",
    "    'Upheld': 'positive',\n",
    "    'Not upheld': 'negative'\n",
    "}\n",
    "\n",
    "# function to create directories if they don't exist\n",
    "def create_directory(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# iterate over the metadata and copy files to the appropriate directories\n",
    "for _, row in df.iterrows():\n",
    "    decision = decision_mapping.get(row['decision'], 'unknown')\n",
    "    dataset = row['set']\n",
    "    plaintext_path = row['plaintext_path']\n",
    "    \n",
    "    # create the target directory path\n",
    "    target_dir = os.path.join(base_dir, dataset, decision)\n",
    "    create_directory(target_dir)\n",
    "    \n",
    "    # define source and target file paths\n",
    "    source_path = os.path.join('./', plaintext_path)\n",
    "    target_path = os.path.join(target_dir, os.path.basename(plaintext_path))\n",
    "    \n",
    "    # copy the file to the target directory\n",
    "    if os.path.exists(source_path):\n",
    "        shutil.copy(source_path, target_path)\n",
    "    else:\n",
    "        print(f\"source file not found: {source_path}\")\n",
    "\n",
    "print(\"files have been organized successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
