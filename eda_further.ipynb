{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Zhile Xu\n",
    "\n",
    "UUN: s2500393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# turn off depreciation warnings and future warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# function to load data\n",
    "def load_data(base_dir):\n",
    "    data = []\n",
    "    labels = []\n",
    "    files = []\n",
    "    for label in ['positive', 'negative']:\n",
    "        for filepath in glob.glob(os.path.join(base_dir, label, '*.txt')):\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                data.append(file.read())\n",
    "                labels.append(1 if label == 'positive' else 0)\n",
    "                files.append(filepath)\n",
    "                \n",
    "    return data, labels, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_data, train_labels, train_files = load_data('data/train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete irrelevant text\n",
    "def remove_irrelevant_text(data):\n",
    "    new_data = []\n",
    "    for text in data:\n",
    "        new_data.append(text.split(\"What I've decided and why\")[0])\n",
    "    return new_data\n",
    "\n",
    "\n",
    "# # calutate the occurence of \"What I've decided and why\" in the text\n",
    "# def count_irrelevant_text(data):\n",
    "#     count = 0\n",
    "#     for text in data:\n",
    "#         if \"What I've decided and why\" in text:\n",
    "#             count += 1\n",
    "#     return count\n",
    "\n",
    "# count_pre = count_irrelevant_text(train_data)\n",
    "# print(\"The number of irrelevant text is: \", count_pre)\n",
    "# # print the number of documents in the training data\n",
    "# print(\"The number of documents in the training data is: \", len(train_data))\n",
    "\n",
    "# # remove irrelevant text\n",
    "# train_data = remove_irrelevant_text(train_data)\n",
    "# count_after = count_irrelevant_text(train_data)\n",
    "# print(\"The number of irrelevant text after removing is: \", count_after)\n",
    "\n",
    "# preprocess the texts\n",
    "def preprocess_texts(texts):\n",
    "    docs = [nlp(text) for text in texts]\n",
    "    return docs\n",
    "\n",
    "# preprocess the training data\n",
    "docs = preprocess_texts(train_data)\n",
    "\n",
    "# separate positive and negative documents\n",
    "positive_docs = [doc for doc, label in zip(docs, train_labels) if label == 1]\n",
    "negative_docs = [doc for doc, label in zip(docs, train_labels) if label == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove stopwords and punctuation\n",
    "def remove_stopwords_punctuation(doc):\n",
    "    # remove stopwords and punctuation\n",
    "    doc = [token for token in doc if not token.is_stop and not token.is_punct]\n",
    "    # remove \"\\n\", \"Mr\", \"Mrs\", \"Miss\" and \"Ms\"\n",
    "    doc = [token for token in doc if token.text not in ['\\n', 'Mr', 'Mrs', 'Miss', 'Ms']]\n",
    "    # remove single characters\n",
    "    doc = [token for token in doc if len(token.text) > 1]\n",
    "    return doc\n",
    "\n",
    "\n",
    "cleaned_docs = [remove_stopwords_punctuation(doc) for doc in docs]\n",
    "positive_cleaned_docs = [remove_stopwords_punctuation(doc) for doc in positive_docs]\n",
    "negative_cleaned_docs = [remove_stopwords_punctuation(doc) for doc in negative_docs]\n",
    "\n",
    "##  Lowercase and lemmatise the tokens\n",
    "def lowercase_and_lemmatise(docs):\n",
    "    lemmatised_docs = []\n",
    "    for doc in docs:\n",
    "        lemmatised_tokens = [token.lemma_.lower() for token in doc]\n",
    "        \n",
    "        lemmatised_docs.append(lemmatised_tokens)\n",
    "    return lemmatised_docs\n",
    "\n",
    "# lowercase and lemmatise the tokens\n",
    "lemmatised_docs = lowercase_and_lemmatise(cleaned_docs)\n",
    "lemmatised_positive_docs = lowercase_and_lemmatise(positive_cleaned_docs)\n",
    "lemmatised_negative_docs = lowercase_and_lemmatise(negative_cleaned_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Topic Model\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "def build_topic_model(docs, dictionary, corpus, num_topics):\n",
    "\n",
    "    # train an LDA model\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, iterations=50,\n",
    "                          num_topics=num_topics, passes=20, random_state=123)\n",
    "\n",
    "    # compute the coherence score\n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=docs, \n",
    "                                     dictionary=dictionary, coherence='c_v')\n",
    "\n",
    "    print(f'Num Topics: {num_topics}, Coherence Score: {coherence_model.get_coherence()}')\n",
    "\n",
    "    return lda_model, coherence_model.get_coherence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function on the whole dataset\n",
    "print(\"Topic model for the whole dataset\")\n",
    "\n",
    "# create a dictionary\n",
    "dictionary = Dictionary(lemmatised_docs)\n",
    "\n",
    "# filter out tokens that appear in less than 5 documents or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "# create a bag-of-words representation of the documents\n",
    "corpus = [dictionary.doc2bow(doc) for doc in lemmatised_docs]\n",
    "\n",
    "coherence = pd.DataFrame(index=range(2, 21), columns=['coherence'])\n",
    "for num_topics in coherence.index:\n",
    "    _, coherence_score = build_topic_model(lemmatised_docs, dictionary, corpus, num_topics)\n",
    "    coherence.loc[num_topics, 'coherence'] = coherence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the optimal number of topics\n",
    "optimal_num_topics = coherence['coherence'].idxmax()\n",
    "print(f'The optimal number of topics is: {optimal_num_topics}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 topics has the highest coherence score\n",
    "# train the LDA model with 16 topics\n",
    "lda_model, _ = build_topic_model(lemmatised_docs, dictionary, corpus, optimal_num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the words in each topic (dont show the whole list)\n",
    "for topic_id in range(optimal_num_topics):\n",
    "    # get the words in the topic\n",
    "    print(f'Topic {topic_id}: {lda_model.print_topic(topic_id)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_topics = {\n",
    "    0: \"Car Insurance (vehicle loss, repair, and accident liability)\",\n",
    "    1: \"Home Appliance Repair Insurance (appliance repairs, warranties, and claim denials)\",\n",
    "    2: \"Property Repair Insurance (certified repairs, home repairs, and property settlements)\",\n",
    "    3: \"Home Repair Insurance (work delays, property loss, and compensation)\",\n",
    "    4: \"Travel Insurance (vehicle delays and travel issues)\",\n",
    "    5: \"Car Insurance (driver and vehicle issues and claim denials)\",\n",
    "    6: \"Home Boiler Insurance (boiler repairs, replacements, and engineer services)\",\n",
    "    7: \"Legal Expense Insurance (legal consultations, disputes, and litigation costs)\",\n",
    "    8: \"Car Insurance (vehicle valuation and settlement)\",\n",
    "    9: \"Home Water Damage Insurance (water leaks, storm damage, and repairs)\",\n",
    "    10: \"Subsidence Insurance (property foundation issues and related compensation)\",\n",
    "    11: \"Insurance Fraud (misrepresentation and claim disputes)\",\n",
    "    12: \"Insurance Premiums and Renewals (premium increases and renewal issues)\",\n",
    "    13: \"Health Insurance (medical treatments, claim denials, and hospital issues)\"\n",
    "}\n",
    "\n",
    "import random\n",
    "random.seed(123)\n",
    "# print 5 document text with their topics\n",
    "for i in range(5):\n",
    "    # choose a random document\n",
    "    doc_id = random.randint(0, len(lemmatised_docs))\n",
    "    # get the document's topic distribution\n",
    "    topic_distribution = lda_model.get_document_topics(corpus[doc_id])\n",
    "    # get the topic with the highest probability\n",
    "    topic_id = max(topic_distribution, key=lambda x: x[1])[0]\n",
    "    # print the document text and the topic\n",
    "    print(f'File: {train_files[doc_id]}')\n",
    "    print(f'Topic: {topic_id} - {dict_topics[topic_id]}')\n",
    "    print(train_data[doc_id][:500])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign topics to the documents\n",
    "topics = [max(lda_model[doc], key=lambda x: x[1])[0] for doc in corpus]\n",
    "\n",
    "# create a dataframe with the topics\n",
    "df_topics = pd.DataFrame({'topic': topics, 'topic_label': train_labels})\n",
    "\n",
    "def plot_topic_distribution(df_topics, dict_topics=dict_topics):\n",
    "    df_topics['topic_label'] = df_topics['topic_label'].map({1: 'Upheld', 0: 'Not Upheld'})\n",
    "    df_topics['topic'] = df_topics['topic'].map(dict_topics)\n",
    "    \n",
    "    # set the aesthetic style of the plots\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # create the count plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.countplot(y='topic', hue='topic_label', data=df_topics, palette='Set1')\n",
    "    \n",
    "    # add titles and labels\n",
    "    plt.title('Distribution of Topics in the Dataset', fontsize=16)\n",
    "    plt.ylabel('Topic', fontsize=14)\n",
    "    plt.xlabel('Count', fontsize=14)\n",
    "    \n",
    "    # customize the legend\n",
    "    plt.legend(title='Topic Label', title_fontsize='13', fontsize='12', loc='lower right')\n",
    "    \n",
    "    # adjust the y-axis tick labels for better readability\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    # display the plot\n",
    "    plt.tight_layout()\n",
    "    # save the plot in the highest quality\n",
    "    plt.savefig('plots/topic-distribution.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_topic_distribution(df_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a word cloud for 3 topics\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def draw_word_cloud(lda_model, topic_id, dict_topics=dict_topics):\n",
    "    # get the words in the topic\n",
    "    words = lda_model.show_topic(topic_id, topn=50)\n",
    "    \n",
    "    # create a dictionary from the words\n",
    "    word_dict = {word: score for word, score in words}\n",
    "    \n",
    "    # create the word cloud\n",
    "    word_cloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_dict)\n",
    "    \n",
    "    # plot the word cloud\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(word_cloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Topic {topic_id}: {dict_topics[topic_id]}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    # save the plot in the highest quality\n",
    "    plt.savefig(f'plots/topic-{topic_id}-word-cloud.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# draw the word cloud for the topics\n",
    "for topic_id in range(4):\n",
    "    draw_word_cloud(lda_model, topic_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the topics \n",
    "pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the named entity recognition\n",
    "def extract_named_entities(docs):\n",
    "    named_entities = []\n",
    "    for doc in docs:\n",
    "        named_entities.extend([ent.text for ent in doc.ents])\n",
    "    return named_entities\n",
    "\n",
    "# extract named entities\n",
    "named_entities = extract_named_entities(docs)\n",
    "\n",
    "# print the categories of named entities\n",
    "named_entity_categories = set([ent.label_ for ent in nlp(' '.join(named_entities)).ents])\n",
    "print(\"categories of named entities: \", named_entity_categories)\n",
    "# {'NORP', 'LOC', 'DATE', 'GPE', 'EVENT', 'PRODUCT', 'LAW', 'PERSON', \n",
    "# 'PERCENT', 'TIME', 'WORK_OF_ART', 'CARDINAL', 'ORDINAL', 'ORG', \n",
    "# 'FAC', 'MONEY', 'LANGUAGE', 'QUANTITY'}\n",
    "\n",
    "# delete language category\n",
    "named_entity_categories.remove('LANGUAGE')\n",
    "print(\"remaining categories: \", named_entity_categories)\n",
    "\n",
    "# extract named entities for positive and negative documents\n",
    "positive_named_entities = extract_named_entities(positive_docs)\n",
    "negative_named_entities = extract_named_entities(negative_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot named entities in different categories for positive and negative documents\n",
    "\n",
    "def plot_named_entities(pos_named_entities, neg_named_entities, category, category_map):\n",
    "    pos_named_entity_counts = Counter(pos_named_entities)\n",
    "    pos_named_entity_df = pd.DataFrame(pos_named_entity_counts.most_common(20), columns=['Named Entity', 'Frequency'])\n",
    "    neg_named_entity_counts = Counter(neg_named_entities)\n",
    "    neg_named_entity_df = pd.DataFrame(neg_named_entity_counts.most_common(20), columns=['Named Entity', 'Frequency'])\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 7))\n",
    "    sns.barplot(x='Frequency', y='Named Entity', data=pos_named_entity_df, ax=axs[0], palette='coolwarm')\n",
    "    sns.barplot(x='Frequency', y='Named Entity', data=neg_named_entity_df, ax=axs[1], palette='coolwarm')\n",
    "    # create a map of category names to more readable names\n",
    "    axs[0].set_title(f'Top 20 Positive {category_map[category]} Named Entities')\n",
    "    axs[1].set_title(f'Top 20 Negative {category_map[category]} Named Entities')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "category_map = {'NORP': 'Nationalities, Religious/Political Groups',\n",
    "                'LOC': 'Locations',\n",
    "                'DATE': 'Dates',\n",
    "                'GPE': 'Countries, Cities, States',\n",
    "                'EVENT': 'Events',\n",
    "                'PRODUCT': 'Products',\n",
    "                'LAW': 'Laws',\n",
    "                'PERSON': 'People',\n",
    "                'PERCENT': 'Percentage',\n",
    "                'TIME': 'Time',\n",
    "                'WORK_OF_ART': 'Works of Art',\n",
    "                'CARDINAL': 'Cardinal Numbers',\n",
    "                'ORDINAL': 'Ordinal Numbers',\n",
    "                'ORG': 'Organizations',\n",
    "                'FAC': 'Facilities',\n",
    "                'MONEY': 'Monetary Values',\n",
    "                'LANGUAGE': 'Languages',\n",
    "                'QUANTITY': 'Measurements'}\n",
    "\n",
    "# plot named entities in different categories for positive and negative documents\n",
    "for category in named_entity_categories:\n",
    "    plot_named_entities([ent.text for ent in nlp(' '.join(positive_named_entities)).ents if ent.label_ == category],\n",
    "                        [ent.text for ent in nlp(' '.join(negative_named_entities)).ents if ent.label_ == category],\n",
    "                        category, category_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
